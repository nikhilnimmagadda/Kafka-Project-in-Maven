[2020-02-03 18:37:08,190] DEBUG preRegister called. Server=com.sun.jmx.mbeanserver.JmxMBeanServer@25618e91, name=log4j:logger=state.change.logger (state.change.logger)
[2020-02-03 18:46:15,348] DEBUG preRegister called. Server=com.sun.jmx.mbeanserver.JmxMBeanServer@25618e91, name=log4j:logger=state.change.logger (state.change.logger)
[2020-02-03 18:47:30,214] TRACE [Controller id=0 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 0 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 18:55:27,302] TRACE [Controller id=0 epoch=1] Changed partition first_topic-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2020-02-03 18:55:27,302] TRACE [Controller id=0 epoch=1] Changed partition first_topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2020-02-03 18:55:27,303] TRACE [Controller id=0 epoch=1] Changed partition first_topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2020-02-03 18:55:27,307] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition first_topic-2 from NonExistentReplica to NewReplica (state.change.logger)
[2020-02-03 18:55:27,307] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition first_topic-1 from NonExistentReplica to NewReplica (state.change.logger)
[2020-02-03 18:55:27,307] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition first_topic-0 from NonExistentReplica to NewReplica (state.change.logger)
[2020-02-03 18:55:27,331] TRACE [Controller id=0 epoch=1] Changed partition first_topic-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger)
[2020-02-03 18:55:27,332] TRACE [Controller id=0 epoch=1] Changed partition first_topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger)
[2020-02-03 18:55:27,332] TRACE [Controller id=0 epoch=1] Changed partition first_topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger)
[2020-02-03 18:55:27,333] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) to broker 0 for partition first_topic-0 (state.change.logger)
[2020-02-03 18:55:27,333] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) to broker 0 for partition first_topic-2 (state.change.logger)
[2020-02-03 18:55:27,333] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) to broker 0 for partition first_topic-1 (state.change.logger)
[2020-02-03 18:55:27,334] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition first_topic-0 (state.change.logger)
[2020-02-03 18:55:27,334] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition first_topic-2 (state.change.logger)
[2020-02-03 18:55:27,334] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition first_topic-1 (state.change.logger)
[2020-02-03 18:55:27,335] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition first_topic-2 from NewReplica to OnlineReplica (state.change.logger)
[2020-02-03 18:55:27,335] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition first_topic-1 from NewReplica to OnlineReplica (state.change.logger)
[2020-02-03 18:55:27,335] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition first_topic-0 from NewReplica to OnlineReplica (state.change.logger)
[2020-02-03 18:55:27,338] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 0 epoch 1 (state.change.logger)
[2020-02-03 18:55:27,338] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 0 epoch 1 (state.change.logger)
[2020-02-03 18:55:27,338] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 0 epoch 1 (state.change.logger)
[2020-02-03 18:55:27,351] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 1 starting the become-leader transition for partition first_topic-0 (state.change.logger)
[2020-02-03 18:55:27,351] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 1 starting the become-leader transition for partition first_topic-1 (state.change.logger)
[2020-02-03 18:55:27,351] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 1 starting the become-leader transition for partition first_topic-2 (state.change.logger)
[2020-02-03 18:55:27,436] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 1 for partition first_topic-0 (last update controller epoch 1) (state.change.logger)
[2020-02-03 18:55:27,450] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 1 for partition first_topic-1 (last update controller epoch 1) (state.change.logger)
[2020-02-03 18:55:27,465] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 1 for partition first_topic-2 (last update controller epoch 1) (state.change.logger)
[2020-02-03 18:55:27,466] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 1 for the become-leader transition for partition first_topic-0 (state.change.logger)
[2020-02-03 18:55:27,466] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 1 for the become-leader transition for partition first_topic-1 (state.change.logger)
[2020-02-03 18:55:27,466] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 1 for the become-leader transition for partition first_topic-2 (state.change.logger)
[2020-02-03 18:55:27,472] TRACE [Controller id=0 epoch=1] Received response {error_code=0,partition_errors=[{topic_name=first_topic,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=first_topic,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=first_topic,partition_index=1,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 1 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 18:55:27,477] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition first_topic-0 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger)
[2020-02-03 18:55:27,477] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition first_topic-2 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger)
[2020-02-03 18:55:27,477] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition first_topic-1 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger)
[2020-02-03 18:55:27,477] TRACE [Controller id=0 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 2 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:00:33,661] TRACE [Controller id=0 epoch=1] Changed partition second_topic-3 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2020-02-03 19:00:33,662] TRACE [Controller id=0 epoch=1] Changed partition second_topic-5 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2020-02-03 19:00:33,663] TRACE [Controller id=0 epoch=1] Changed partition second_topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2020-02-03 19:00:33,663] TRACE [Controller id=0 epoch=1] Changed partition second_topic-2 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2020-02-03 19:00:33,663] TRACE [Controller id=0 epoch=1] Changed partition second_topic-1 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2020-02-03 19:00:33,663] TRACE [Controller id=0 epoch=1] Changed partition second_topic-4 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2020-02-03 19:00:33,663] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-3 from NonExistentReplica to NewReplica (state.change.logger)
[2020-02-03 19:00:33,663] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-1 from NonExistentReplica to NewReplica (state.change.logger)
[2020-02-03 19:00:33,664] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-5 from NonExistentReplica to NewReplica (state.change.logger)
[2020-02-03 19:00:33,664] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-0 from NonExistentReplica to NewReplica (state.change.logger)
[2020-02-03 19:00:33,664] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-2 from NonExistentReplica to NewReplica (state.change.logger)
[2020-02-03 19:00:33,664] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-4 from NonExistentReplica to NewReplica (state.change.logger)
[2020-02-03 19:00:33,681] TRACE [Controller id=0 epoch=1] Changed partition second_topic-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger)
[2020-02-03 19:00:33,681] TRACE [Controller id=0 epoch=1] Changed partition second_topic-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger)
[2020-02-03 19:00:33,681] TRACE [Controller id=0 epoch=1] Changed partition second_topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger)
[2020-02-03 19:00:33,681] TRACE [Controller id=0 epoch=1] Changed partition second_topic-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger)
[2020-02-03 19:00:33,681] TRACE [Controller id=0 epoch=1] Changed partition second_topic-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger)
[2020-02-03 19:00:33,681] TRACE [Controller id=0 epoch=1] Changed partition second_topic-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger)
[2020-02-03 19:00:33,681] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) to broker 0 for partition second_topic-2 (state.change.logger)
[2020-02-03 19:00:33,682] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) to broker 0 for partition second_topic-4 (state.change.logger)
[2020-02-03 19:00:33,682] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) to broker 0 for partition second_topic-1 (state.change.logger)
[2020-02-03 19:00:33,682] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) to broker 0 for partition second_topic-3 (state.change.logger)
[2020-02-03 19:00:33,682] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) to broker 0 for partition second_topic-0 (state.change.logger)
[2020-02-03 19:00:33,682] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) to broker 0 for partition second_topic-5 (state.change.logger)
[2020-02-03 19:00:33,682] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:00:33,682] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:00:33,682] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:00:33,682] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:00:33,682] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:00:33,683] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:00:33,683] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 0 epoch 1 (state.change.logger)
[2020-02-03 19:00:33,684] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-3 from NewReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:00:33,684] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 0 epoch 1 (state.change.logger)
[2020-02-03 19:00:33,684] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-1 from NewReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:00:33,684] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-5 from NewReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:00:33,684] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-0 from NewReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:00:33,684] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-2 from NewReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:00:33,684] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 0 epoch 1 (state.change.logger)
[2020-02-03 19:00:33,684] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-4 from NewReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:00:33,684] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 0 epoch 1 (state.change.logger)
[2020-02-03 19:00:33,684] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 0 epoch 1 (state.change.logger)
[2020-02-03 19:00:33,684] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 0 epoch 1 (state.change.logger)
[2020-02-03 19:00:33,689] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 epoch 1 starting the become-leader transition for partition second_topic-4 (state.change.logger)
[2020-02-03 19:00:33,689] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 epoch 1 starting the become-leader transition for partition second_topic-1 (state.change.logger)
[2020-02-03 19:00:33,689] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 epoch 1 starting the become-leader transition for partition second_topic-5 (state.change.logger)
[2020-02-03 19:00:33,689] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 epoch 1 starting the become-leader transition for partition second_topic-2 (state.change.logger)
[2020-02-03 19:00:33,689] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 epoch 1 starting the become-leader transition for partition second_topic-3 (state.change.logger)
[2020-02-03 19:00:33,689] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 epoch 1 starting the become-leader transition for partition second_topic-0 (state.change.logger)
[2020-02-03 19:00:33,712] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 3 for partition second_topic-4 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:00:33,727] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 3 for partition second_topic-1 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:00:33,743] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 3 for partition second_topic-5 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:00:33,759] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 3 for partition second_topic-2 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:00:33,777] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 3 for partition second_topic-3 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:00:33,793] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 3 for partition second_topic-0 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:00:33,793] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 0 epoch 1 for the become-leader transition for partition second_topic-4 (state.change.logger)
[2020-02-03 19:00:33,793] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 0 epoch 1 for the become-leader transition for partition second_topic-1 (state.change.logger)
[2020-02-03 19:00:33,793] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 0 epoch 1 for the become-leader transition for partition second_topic-5 (state.change.logger)
[2020-02-03 19:00:33,793] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 0 epoch 1 for the become-leader transition for partition second_topic-2 (state.change.logger)
[2020-02-03 19:00:33,793] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 0 epoch 1 for the become-leader transition for partition second_topic-3 (state.change.logger)
[2020-02-03 19:00:33,793] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 0 epoch 1 for the become-leader transition for partition second_topic-0 (state.change.logger)
[2020-02-03 19:00:33,794] TRACE [Controller id=0 epoch=1] Received response {error_code=0,partition_errors=[{topic_name=second_topic,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=4,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=3,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=5,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 3 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:00:33,795] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition second_topic-2 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger)
[2020-02-03 19:00:33,795] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition second_topic-4 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger)
[2020-02-03 19:00:33,795] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition second_topic-1 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger)
[2020-02-03 19:00:33,795] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition second_topic-3 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger)
[2020-02-03 19:00:33,795] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition second_topic-0 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger)
[2020-02-03 19:00:33,795] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition second_topic-5 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger)
[2020-02-03 19:00:33,796] TRACE [Controller id=0 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 4 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:01:18,235] TRACE [Controller id=0 epoch=1] Changed partition second_topic-3 state from OnlinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:01:18,235] TRACE [Controller id=0 epoch=1] Changed partition second_topic-5 state from OnlinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:01:18,235] TRACE [Controller id=0 epoch=1] Changed partition second_topic-0 state from OnlinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:01:18,235] TRACE [Controller id=0 epoch=1] Changed partition second_topic-2 state from OnlinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:01:18,235] TRACE [Controller id=0 epoch=1] Changed partition second_topic-1 state from OnlinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:01:18,235] TRACE [Controller id=0 epoch=1] Changed partition second_topic-4 state from OnlinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:01:18,236] TRACE [Controller id=0 epoch=1] Changed partition second_topic-3 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:01:18,236] TRACE [Controller id=0 epoch=1] Changed partition second_topic-5 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:01:18,236] TRACE [Controller id=0 epoch=1] Changed partition second_topic-0 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:01:18,236] TRACE [Controller id=0 epoch=1] Changed partition second_topic-2 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:01:18,236] TRACE [Controller id=0 epoch=1] Changed partition second_topic-1 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:01:18,236] TRACE [Controller id=0 epoch=1] Changed partition second_topic-4 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:01:18,237] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:01:18,237] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:01:18,237] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:01:18,237] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:01:18,237] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:01:18,237] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:01:18,240] TRACE [Broker id=0] Deleted partition second_topic-2 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 5 (state.change.logger)
[2020-02-03 19:01:18,240] TRACE [Broker id=0] Deleted partition second_topic-4 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 5 (state.change.logger)
[2020-02-03 19:01:18,240] TRACE [Broker id=0] Deleted partition second_topic-1 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 5 (state.change.logger)
[2020-02-03 19:01:18,240] TRACE [Broker id=0] Deleted partition second_topic-3 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 5 (state.change.logger)
[2020-02-03 19:01:18,240] TRACE [Broker id=0] Deleted partition second_topic-0 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 5 (state.change.logger)
[2020-02-03 19:01:18,240] TRACE [Broker id=0] Deleted partition second_topic-5 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 5 (state.change.logger)
[2020-02-03 19:01:18,242] TRACE [Controller id=0 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 5 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:01:18,255] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-3 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:01:18,255] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-5 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:01:18,255] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-0 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:01:18,255] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-2 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:01:18,255] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-1 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:01:18,255] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-4 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:01:18,257] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-3 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:01:18,257] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-1 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:01:18,257] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-5 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:01:18,257] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-0 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:01:18,257] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-2 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:01:18,257] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-4 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:01:18,261] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:01:18,262] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:01:18,262] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:01:18,262] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:01:18,262] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:01:18,262] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:01:18,262] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:01:18,262] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:01:18,262] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:01:18,262] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:01:18,263] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:01:18,263] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:01:18,264] TRACE [Controller id=0 epoch=1] Received response {error_code=0,partition_errors=[{topic_name=second_topic,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=4,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=3,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=5,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request STOP_REPLICA with correlation id 6 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:01:18,266] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:01:18,282] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-3 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-3 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3.029038a1f551422fae593f5a682009c5-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3.029038a1f551422fae593f5a682009c5-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:01:18,282] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:01:18,291] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-5 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-5 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5.a267cab21b244e8abe8cf0fd1cae2e43-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5.a267cab21b244e8abe8cf0fd1cae2e43-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:01:18,292] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:01:18,301] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-0 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-0 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0.2b713daf8c474ac88076293e7803a634-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0.2b713daf8c474ac88076293e7803a634-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:01:18,301] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:01:18,310] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-2 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-2 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2.acea68634d0a4812914ec9beee08f713-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2.acea68634d0a4812914ec9beee08f713-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:01:18,310] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:01:18,319] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-1 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-1 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1.282335aa269f4b98bf1bfc972d85cc74-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1.282335aa269f4b98bf1bfc972d85cc74-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:01:18,319] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:01:18,329] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-4 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-4 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4.873c67e1035841f6a3e18255c76737b4-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4.873c67e1035841f6a3e18255c76737b4-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:01:18,331] TRACE [Controller id=0 epoch=1] Received response {error_code=0,partition_errors=[{topic_name=second_topic,partition_index=2,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=4,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=1,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=3,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=0,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=5,error_code=56,_tagged_fields={}}],_tagged_fields={}} for request STOP_REPLICA with correlation id 7 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:01:18,333] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-3 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:01:18,333] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-1 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:01:18,333] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-5 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:01:18,333] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-0 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:01:18,333] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-2 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:01:18,333] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition second_topic-4 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:08:35,373] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-3 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:08:35,375] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition first_topic-1 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:08:35,376] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-4 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:08:35,376] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-5 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:08:35,376] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-0 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:08:35,377] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-2 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:08:35,377] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition first_topic-0 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:08:35,377] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition first_topic-2 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:08:35,378] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-1 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:08:35,379] TRACE [Controller id=0 epoch=2] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition first_topic-0 (state.change.logger)
[2020-02-03 19:08:35,379] TRACE [Controller id=0 epoch=2] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=1, leader=-1, leaderEpoch=2, isr=[0], zkVersion=2, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-2 (state.change.logger)
[2020-02-03 19:08:35,379] TRACE [Controller id=0 epoch=2] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=1, leader=-1, leaderEpoch=2, isr=[0], zkVersion=2, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-4 (state.change.logger)
[2020-02-03 19:08:35,380] TRACE [Controller id=0 epoch=2] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=1, leader=-1, leaderEpoch=2, isr=[0], zkVersion=2, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-1 (state.change.logger)
[2020-02-03 19:08:35,380] TRACE [Controller id=0 epoch=2] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition first_topic-2 (state.change.logger)
[2020-02-03 19:08:35,380] TRACE [Controller id=0 epoch=2] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition first_topic-1 (state.change.logger)
[2020-02-03 19:08:35,380] TRACE [Controller id=0 epoch=2] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=1, leader=-1, leaderEpoch=2, isr=[0], zkVersion=2, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-3 (state.change.logger)
[2020-02-03 19:08:35,380] TRACE [Controller id=0 epoch=2] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=1, leader=-1, leaderEpoch=2, isr=[0], zkVersion=2, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-0 (state.change.logger)
[2020-02-03 19:08:35,380] TRACE [Controller id=0 epoch=2] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=1, leader=-1, leaderEpoch=2, isr=[0], zkVersion=2, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-5 (state.change.logger)
[2020-02-03 19:08:35,383] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition first_topic-0 (state.change.logger)
[2020-02-03 19:08:35,383] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:08:35,383] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:08:35,383] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:08:35,383] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition first_topic-2 (state.change.logger)
[2020-02-03 19:08:35,383] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition first_topic-1 (state.change.logger)
[2020-02-03 19:08:35,383] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:08:35,383] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:08:35,384] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:08:35,404] TRACE [Controller id=0 epoch=2] Changed partition second_topic-3 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:08:35,404] TRACE [Controller id=0 epoch=2] Changed partition second_topic-5 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:08:35,404] TRACE [Controller id=0 epoch=2] Changed partition second_topic-0 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:08:35,404] TRACE [Controller id=0 epoch=2] Changed partition second_topic-2 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:08:35,404] TRACE [Controller id=0 epoch=2] Changed partition second_topic-1 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:08:35,404] TRACE [Controller id=0 epoch=2] Changed partition second_topic-4 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:08:35,406] TRACE [Controller id=0 epoch=2] Changed partition second_topic-3 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:08:35,406] TRACE [Controller id=0 epoch=2] Changed partition second_topic-5 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:08:35,406] TRACE [Controller id=0 epoch=2] Changed partition second_topic-0 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:08:35,406] TRACE [Controller id=0 epoch=2] Changed partition second_topic-2 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:08:35,406] TRACE [Controller id=0 epoch=2] Changed partition second_topic-1 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:08:35,406] TRACE [Controller id=0 epoch=2] Changed partition second_topic-4 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:08:35,407] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:08:35,408] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:08:35,408] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:08:35,408] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:08:35,408] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:08:35,408] TRACE [Controller id=0 epoch=2] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=1, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:08:35,440] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-3 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:08:35,441] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-5 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:08:35,441] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-0 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:08:35,441] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-2 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:08:35,441] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-1 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:08:35,441] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-4 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:08:35,444] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-3 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:08:35,444] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-1 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:08:35,444] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-5 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:08:35,444] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-0 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:08:35,444] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-2 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:08:35,444] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-4 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:08:35,463] TRACE [Controller id=0 epoch=2] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 0 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:08:35,471] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 2 (state.change.logger)
[2020-02-03 19:08:35,471] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 2 (state.change.logger)
[2020-02-03 19:08:35,471] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 2 (state.change.logger)
[2020-02-03 19:08:35,471] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=1, leader=-1, leaderEpoch=2, isr=[0], zkVersion=2, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 2 (state.change.logger)
[2020-02-03 19:08:35,471] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=1, leader=-1, leaderEpoch=2, isr=[0], zkVersion=2, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 2 (state.change.logger)
[2020-02-03 19:08:35,471] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=1, leader=-1, leaderEpoch=2, isr=[0], zkVersion=2, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 2 (state.change.logger)
[2020-02-03 19:08:35,471] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=1, leader=-1, leaderEpoch=2, isr=[0], zkVersion=2, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 2 (state.change.logger)
[2020-02-03 19:08:35,471] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=1, leader=-1, leaderEpoch=2, isr=[0], zkVersion=2, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 2 (state.change.logger)
[2020-02-03 19:08:35,471] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=1, leader=-1, leaderEpoch=2, isr=[0], zkVersion=2, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 2 (state.change.logger)
[2020-02-03 19:08:35,491] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 2 starting the become-leader transition for partition first_topic-0 (state.change.logger)
[2020-02-03 19:08:35,491] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 2 starting the become-leader transition for partition first_topic-1 (state.change.logger)
[2020-02-03 19:08:35,491] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 2 starting the become-leader transition for partition first_topic-2 (state.change.logger)
[2020-02-03 19:08:35,530] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 2 with correlation id 1 for partition first_topic-0 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:08:35,540] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 2 with correlation id 1 for partition first_topic-1 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:08:35,549] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 2 with correlation id 1 for partition first_topic-2 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:08:35,550] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 2 for the become-leader transition for partition first_topic-0 (state.change.logger)
[2020-02-03 19:08:35,550] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 2 for the become-leader transition for partition first_topic-1 (state.change.logger)
[2020-02-03 19:08:35,550] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 2 for the become-leader transition for partition first_topic-2 (state.change.logger)
[2020-02-03 19:08:35,550] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 2 starting the become-follower transition for partition second_topic-4 with leader -1 (state.change.logger)
[2020-02-03 19:08:35,550] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 2 starting the become-follower transition for partition second_topic-1 with leader -1 (state.change.logger)
[2020-02-03 19:08:35,550] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 2 starting the become-follower transition for partition second_topic-5 with leader -1 (state.change.logger)
[2020-02-03 19:08:35,550] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 2 starting the become-follower transition for partition second_topic-2 with leader -1 (state.change.logger)
[2020-02-03 19:08:35,550] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 2 starting the become-follower transition for partition second_topic-3 with leader -1 (state.change.logger)
[2020-02-03 19:08:35,550] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 2 starting the become-follower transition for partition second_topic-0 with leader -1 (state.change.logger)
[2020-02-03 19:08:35,552] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 2 for partition second_topic-4 (last update controller epoch 1) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:08:35,555] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 2 for partition second_topic-1 (last update controller epoch 1) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:08:35,557] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 2 for partition second_topic-5 (last update controller epoch 1) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:08:35,559] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 2 for partition second_topic-2 (last update controller epoch 1) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:08:35,562] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 2 for partition second_topic-3 (last update controller epoch 1) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:08:35,564] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 2 for partition second_topic-0 (last update controller epoch 1) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:08:35,577] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 2 for the become-follower transition for partition second_topic-4 with leader -1 (state.change.logger)
[2020-02-03 19:08:35,577] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 2 for the become-follower transition for partition second_topic-1 with leader -1 (state.change.logger)
[2020-02-03 19:08:35,577] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 2 for the become-follower transition for partition second_topic-5 with leader -1 (state.change.logger)
[2020-02-03 19:08:35,577] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 2 for the become-follower transition for partition second_topic-2 with leader -1 (state.change.logger)
[2020-02-03 19:08:35,577] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 2 for the become-follower transition for partition second_topic-3 with leader -1 (state.change.logger)
[2020-02-03 19:08:35,577] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 2 for the become-follower transition for partition second_topic-0 with leader -1 (state.change.logger)
[2020-02-03 19:08:35,584] TRACE [Controller id=0 epoch=2] Received response {error_code=0,partition_errors=[{topic_name=first_topic,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=4,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=first_topic,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=first_topic,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=3,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=5,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 1 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:08:35,593] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition first_topic-0 in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 2 (state.change.logger)
[2020-02-03 19:08:35,593] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition first_topic-2 in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 2 (state.change.logger)
[2020-02-03 19:08:35,593] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition first_topic-1 in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 2 (state.change.logger)
[2020-02-03 19:08:35,595] TRACE [Broker id=0] Deleted partition second_topic-2 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 2 (state.change.logger)
[2020-02-03 19:08:35,595] TRACE [Broker id=0] Deleted partition second_topic-4 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 2 (state.change.logger)
[2020-02-03 19:08:35,595] TRACE [Broker id=0] Deleted partition second_topic-1 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 2 (state.change.logger)
[2020-02-03 19:08:35,595] TRACE [Broker id=0] Deleted partition second_topic-3 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 2 (state.change.logger)
[2020-02-03 19:08:35,595] TRACE [Broker id=0] Deleted partition second_topic-0 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 2 (state.change.logger)
[2020-02-03 19:08:35,595] TRACE [Broker id=0] Deleted partition second_topic-5 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 2 (state.change.logger)
[2020-02-03 19:08:35,597] TRACE [Controller id=0 epoch=2] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 2 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:08:35,600] TRACE [Broker id=0] Deleted partition second_topic-2 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 3 (state.change.logger)
[2020-02-03 19:08:35,600] TRACE [Broker id=0] Deleted partition second_topic-4 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 3 (state.change.logger)
[2020-02-03 19:08:35,600] TRACE [Broker id=0] Deleted partition second_topic-1 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 3 (state.change.logger)
[2020-02-03 19:08:35,600] TRACE [Broker id=0] Deleted partition second_topic-3 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 3 (state.change.logger)
[2020-02-03 19:08:35,600] TRACE [Broker id=0] Deleted partition second_topic-0 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 3 (state.change.logger)
[2020-02-03 19:08:35,600] TRACE [Broker id=0] Deleted partition second_topic-5 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 2 with correlation id 3 (state.change.logger)
[2020-02-03 19:08:35,601] TRACE [Controller id=0 epoch=2] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 3 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:08:35,607] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:08:35,608] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:08:35,608] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:08:35,608] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:08:35,608] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:08:35,608] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:08:35,608] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:08:35,608] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:08:35,608] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:08:35,609] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:08:35,609] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:08:35,609] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:08:35,611] TRACE [Controller id=0 epoch=2] Received response {error_code=0,partition_errors=[{topic_name=second_topic,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=4,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=3,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=5,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request STOP_REPLICA with correlation id 4 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:08:35,612] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:08:35,628] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-3 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-3 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3.c9ffdfbd092c4c159265441d83a9aebf-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3.c9ffdfbd092c4c159265441d83a9aebf-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:08:35,628] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:08:35,638] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-5 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-5 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5.659b4e71f7ed41d0969e894139987e6d-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5.659b4e71f7ed41d0969e894139987e6d-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:08:35,638] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:08:35,648] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-0 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-0 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0.69cd78e8bafa47bb942aa9b7e36f1d57-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0.69cd78e8bafa47bb942aa9b7e36f1d57-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:08:35,648] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:08:35,658] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-2 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-2 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2.fbe0585d491844a7aca5bbb30444dc0d-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2.fbe0585d491844a7aca5bbb30444dc0d-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:08:35,658] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:08:35,667] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-1 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-1 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1.5356b2ad8c0e426d963e7978e3225ab8-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1.5356b2ad8c0e426d963e7978e3225ab8-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:08:35,668] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:08:35,678] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-4 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-4 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4.18b0a6edbc114131949878da8cf25b9d-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4.18b0a6edbc114131949878da8cf25b9d-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:08:35,680] TRACE [Controller id=0 epoch=2] Received response {error_code=0,partition_errors=[{topic_name=second_topic,partition_index=2,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=4,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=1,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=3,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=0,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=5,error_code=56,_tagged_fields={}}],_tagged_fields={}} for request STOP_REPLICA with correlation id 5 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:08:35,683] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-3 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:08:35,683] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-1 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:08:35,683] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-5 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:08:35,683] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-0 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:08:35,683] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-2 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:08:35,683] TRACE [Controller id=0 epoch=2] Changed state of replica 0 for partition second_topic-4 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:09:22,372] DEBUG preRegister called. Server=com.sun.jmx.mbeanserver.JmxMBeanServer@25618e91, name=log4j:logger=state.change.logger (state.change.logger)
[2020-02-03 19:09:41,158] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-3 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:09:41,161] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition first_topic-1 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:09:41,162] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-4 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:09:41,162] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-5 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:09:41,163] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-0 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:09:41,163] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-2 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:09:41,164] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition first_topic-0 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:09:41,164] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition first_topic-2 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:09:41,164] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-1 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:09:41,166] TRACE [Controller id=0 epoch=3] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition first_topic-0 (state.change.logger)
[2020-02-03 19:09:41,166] TRACE [Controller id=0 epoch=3] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=2, leader=-1, leaderEpoch=4, isr=[0], zkVersion=4, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-2 (state.change.logger)
[2020-02-03 19:09:41,166] TRACE [Controller id=0 epoch=3] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=2, leader=-1, leaderEpoch=4, isr=[0], zkVersion=4, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-4 (state.change.logger)
[2020-02-03 19:09:41,166] TRACE [Controller id=0 epoch=3] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=2, leader=-1, leaderEpoch=4, isr=[0], zkVersion=4, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-1 (state.change.logger)
[2020-02-03 19:09:41,166] TRACE [Controller id=0 epoch=3] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition first_topic-2 (state.change.logger)
[2020-02-03 19:09:41,166] TRACE [Controller id=0 epoch=3] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition first_topic-1 (state.change.logger)
[2020-02-03 19:09:41,166] TRACE [Controller id=0 epoch=3] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=2, leader=-1, leaderEpoch=4, isr=[0], zkVersion=4, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-3 (state.change.logger)
[2020-02-03 19:09:41,166] TRACE [Controller id=0 epoch=3] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=2, leader=-1, leaderEpoch=4, isr=[0], zkVersion=4, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-0 (state.change.logger)
[2020-02-03 19:09:41,166] TRACE [Controller id=0 epoch=3] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=2, leader=-1, leaderEpoch=4, isr=[0], zkVersion=4, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-5 (state.change.logger)
[2020-02-03 19:09:41,168] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition first_topic-0 (state.change.logger)
[2020-02-03 19:09:41,168] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=2, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:09:41,169] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=2, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:09:41,169] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=2, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:09:41,169] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition first_topic-2 (state.change.logger)
[2020-02-03 19:09:41,169] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition first_topic-1 (state.change.logger)
[2020-02-03 19:09:41,169] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=2, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:09:41,169] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=2, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:09:41,169] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=2, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:09:41,188] TRACE [Controller id=0 epoch=3] Changed partition second_topic-3 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:09:41,188] TRACE [Controller id=0 epoch=3] Changed partition second_topic-5 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:09:41,189] TRACE [Controller id=0 epoch=3] Changed partition second_topic-0 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:09:41,189] TRACE [Controller id=0 epoch=3] Changed partition second_topic-2 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:09:41,189] TRACE [Controller id=0 epoch=3] Changed partition second_topic-1 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:09:41,189] TRACE [Controller id=0 epoch=3] Changed partition second_topic-4 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:09:41,190] TRACE [Controller id=0 epoch=3] Changed partition second_topic-3 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:09:41,191] TRACE [Controller id=0 epoch=3] Changed partition second_topic-5 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:09:41,191] TRACE [Controller id=0 epoch=3] Changed partition second_topic-0 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:09:41,191] TRACE [Controller id=0 epoch=3] Changed partition second_topic-2 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:09:41,191] TRACE [Controller id=0 epoch=3] Changed partition second_topic-1 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:09:41,191] TRACE [Controller id=0 epoch=3] Changed partition second_topic-4 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:09:41,192] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=2, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:09:41,192] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=2, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:09:41,192] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=2, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:09:41,192] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=2, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:09:41,192] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=2, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:09:41,192] TRACE [Controller id=0 epoch=3] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=2, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:09:41,230] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-3 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:09:41,230] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-5 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:09:41,230] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-0 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:09:41,230] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-2 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:09:41,230] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-1 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:09:41,230] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-4 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:09:41,233] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-3 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:09:41,233] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-1 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:09:41,233] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-5 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:09:41,233] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-0 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:09:41,233] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-2 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:09:41,233] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-4 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:09:41,241] TRACE [Controller id=0 epoch=3] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 0 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:09:41,249] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 3 (state.change.logger)
[2020-02-03 19:09:41,249] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 3 (state.change.logger)
[2020-02-03 19:09:41,249] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 3 (state.change.logger)
[2020-02-03 19:09:41,249] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=2, leader=-1, leaderEpoch=4, isr=[0], zkVersion=4, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 3 (state.change.logger)
[2020-02-03 19:09:41,249] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=2, leader=-1, leaderEpoch=4, isr=[0], zkVersion=4, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 3 (state.change.logger)
[2020-02-03 19:09:41,249] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=2, leader=-1, leaderEpoch=4, isr=[0], zkVersion=4, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 3 (state.change.logger)
[2020-02-03 19:09:41,249] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=2, leader=-1, leaderEpoch=4, isr=[0], zkVersion=4, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 3 (state.change.logger)
[2020-02-03 19:09:41,249] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=2, leader=-1, leaderEpoch=4, isr=[0], zkVersion=4, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 3 (state.change.logger)
[2020-02-03 19:09:41,249] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=2, leader=-1, leaderEpoch=4, isr=[0], zkVersion=4, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 3 (state.change.logger)
[2020-02-03 19:09:41,267] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 3 starting the become-leader transition for partition first_topic-0 (state.change.logger)
[2020-02-03 19:09:41,268] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 3 starting the become-leader transition for partition first_topic-1 (state.change.logger)
[2020-02-03 19:09:41,268] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 3 starting the become-leader transition for partition first_topic-2 (state.change.logger)
[2020-02-03 19:09:41,309] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 3 with correlation id 1 for partition first_topic-0 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:09:41,321] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 3 with correlation id 1 for partition first_topic-1 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:09:41,329] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 3 with correlation id 1 for partition first_topic-2 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:09:41,329] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 3 for the become-leader transition for partition first_topic-0 (state.change.logger)
[2020-02-03 19:09:41,329] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 3 for the become-leader transition for partition first_topic-1 (state.change.logger)
[2020-02-03 19:09:41,329] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 3 for the become-leader transition for partition first_topic-2 (state.change.logger)
[2020-02-03 19:09:41,330] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 3 starting the become-follower transition for partition second_topic-4 with leader -1 (state.change.logger)
[2020-02-03 19:09:41,330] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 3 starting the become-follower transition for partition second_topic-1 with leader -1 (state.change.logger)
[2020-02-03 19:09:41,330] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 3 starting the become-follower transition for partition second_topic-5 with leader -1 (state.change.logger)
[2020-02-03 19:09:41,330] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 3 starting the become-follower transition for partition second_topic-2 with leader -1 (state.change.logger)
[2020-02-03 19:09:41,330] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 3 starting the become-follower transition for partition second_topic-3 with leader -1 (state.change.logger)
[2020-02-03 19:09:41,330] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 3 starting the become-follower transition for partition second_topic-0 with leader -1 (state.change.logger)
[2020-02-03 19:09:41,332] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 3 for partition second_topic-4 (last update controller epoch 2) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:09:41,334] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 3 for partition second_topic-1 (last update controller epoch 2) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:09:41,336] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 3 for partition second_topic-5 (last update controller epoch 2) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:09:41,338] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 3 for partition second_topic-2 (last update controller epoch 2) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:09:41,340] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 3 for partition second_topic-3 (last update controller epoch 2) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:09:41,342] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 3 for partition second_topic-0 (last update controller epoch 2) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:09:41,355] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 3 for the become-follower transition for partition second_topic-4 with leader -1 (state.change.logger)
[2020-02-03 19:09:41,355] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 3 for the become-follower transition for partition second_topic-1 with leader -1 (state.change.logger)
[2020-02-03 19:09:41,355] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 3 for the become-follower transition for partition second_topic-5 with leader -1 (state.change.logger)
[2020-02-03 19:09:41,355] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 3 for the become-follower transition for partition second_topic-2 with leader -1 (state.change.logger)
[2020-02-03 19:09:41,356] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 3 for the become-follower transition for partition second_topic-3 with leader -1 (state.change.logger)
[2020-02-03 19:09:41,356] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 3 for the become-follower transition for partition second_topic-0 with leader -1 (state.change.logger)
[2020-02-03 19:09:41,363] TRACE [Controller id=0 epoch=3] Received response {error_code=0,partition_errors=[{topic_name=first_topic,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=4,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=first_topic,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=first_topic,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=3,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=5,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 1 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:09:41,369] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition first_topic-0 in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 2 (state.change.logger)
[2020-02-03 19:09:41,370] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition first_topic-2 in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 2 (state.change.logger)
[2020-02-03 19:09:41,370] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition first_topic-1 in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 2 (state.change.logger)
[2020-02-03 19:09:41,371] TRACE [Broker id=0] Deleted partition second_topic-2 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 2 (state.change.logger)
[2020-02-03 19:09:41,371] TRACE [Broker id=0] Deleted partition second_topic-4 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 2 (state.change.logger)
[2020-02-03 19:09:41,371] TRACE [Broker id=0] Deleted partition second_topic-1 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 2 (state.change.logger)
[2020-02-03 19:09:41,371] TRACE [Broker id=0] Deleted partition second_topic-3 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 2 (state.change.logger)
[2020-02-03 19:09:41,371] TRACE [Broker id=0] Deleted partition second_topic-0 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 2 (state.change.logger)
[2020-02-03 19:09:41,371] TRACE [Broker id=0] Deleted partition second_topic-5 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 2 (state.change.logger)
[2020-02-03 19:09:41,374] TRACE [Controller id=0 epoch=3] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 2 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:09:41,376] TRACE [Broker id=0] Deleted partition second_topic-2 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 3 (state.change.logger)
[2020-02-03 19:09:41,376] TRACE [Broker id=0] Deleted partition second_topic-4 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 3 (state.change.logger)
[2020-02-03 19:09:41,376] TRACE [Broker id=0] Deleted partition second_topic-1 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 3 (state.change.logger)
[2020-02-03 19:09:41,376] TRACE [Broker id=0] Deleted partition second_topic-3 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 3 (state.change.logger)
[2020-02-03 19:09:41,376] TRACE [Broker id=0] Deleted partition second_topic-0 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 3 (state.change.logger)
[2020-02-03 19:09:41,376] TRACE [Broker id=0] Deleted partition second_topic-5 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 3 with correlation id 3 (state.change.logger)
[2020-02-03 19:09:41,377] TRACE [Controller id=0 epoch=3] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 3 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:09:41,382] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:09:41,383] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:09:41,383] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:09:41,384] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:09:41,384] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:09:41,384] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:09:41,384] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:09:41,384] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:09:41,384] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:09:41,384] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:09:41,384] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:09:41,384] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:09:41,386] TRACE [Controller id=0 epoch=3] Received response {error_code=0,partition_errors=[{topic_name=second_topic,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=4,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=3,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=5,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request STOP_REPLICA with correlation id 4 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:09:41,387] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:09:41,402] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-3 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-3 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3.479640efd7984a67b7c4dd4748f83f91-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3.479640efd7984a67b7c4dd4748f83f91-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:09:41,403] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:09:41,412] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-5 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-5 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5.10f109c1352743d9a77015da081012d7-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5.10f109c1352743d9a77015da081012d7-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:09:41,412] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:09:41,434] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-0 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-0 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0.1a23b9ed8f4846d9a626f34100d1ecd9-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0.1a23b9ed8f4846d9a626f34100d1ecd9-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:09:41,434] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:09:41,461] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-2 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-2 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2.6efb999944fd4c5094b7b2af0bb09cf9-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2.6efb999944fd4c5094b7b2af0bb09cf9-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:09:41,462] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:09:41,480] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-1 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-1 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1.c6c340a7a3144597a32f000d6d8710fc-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1.c6c340a7a3144597a32f000d6d8710fc-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:09:41,481] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:09:41,497] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-4 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-4 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4.0f6750f0218143fc942692c8182a26d8-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4.0f6750f0218143fc942692c8182a26d8-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:09:41,500] TRACE [Controller id=0 epoch=3] Received response {error_code=0,partition_errors=[{topic_name=second_topic,partition_index=2,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=4,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=1,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=3,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=0,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=5,error_code=56,_tagged_fields={}}],_tagged_fields={}} for request STOP_REPLICA with correlation id 5 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:09:41,506] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-3 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:09:41,506] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-1 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:09:41,506] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-5 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:09:41,506] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-0 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:09:41,506] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-2 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:09:41,506] TRACE [Controller id=0 epoch=3] Changed state of replica 0 for partition second_topic-4 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:10:06,623] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-3 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:10:06,626] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition first_topic-1 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:10:06,626] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-4 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:10:06,627] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-5 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:10:06,627] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-0 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:10:06,628] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-2 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:10:06,628] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition first_topic-0 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:10:06,628] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition first_topic-2 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:10:06,629] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-1 from OnlineReplica to OnlineReplica (state.change.logger)
[2020-02-03 19:10:06,631] TRACE [Controller id=0 epoch=4] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition first_topic-0 (state.change.logger)
[2020-02-03 19:10:06,631] TRACE [Controller id=0 epoch=4] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=3, leader=-1, leaderEpoch=6, isr=[0], zkVersion=6, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-2 (state.change.logger)
[2020-02-03 19:10:06,631] TRACE [Controller id=0 epoch=4] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=3, leader=-1, leaderEpoch=6, isr=[0], zkVersion=6, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-4 (state.change.logger)
[2020-02-03 19:10:06,631] TRACE [Controller id=0 epoch=4] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=3, leader=-1, leaderEpoch=6, isr=[0], zkVersion=6, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-1 (state.change.logger)
[2020-02-03 19:10:06,631] TRACE [Controller id=0 epoch=4] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition first_topic-2 (state.change.logger)
[2020-02-03 19:10:06,631] TRACE [Controller id=0 epoch=4] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition first_topic-1 (state.change.logger)
[2020-02-03 19:10:06,631] TRACE [Controller id=0 epoch=4] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=3, leader=-1, leaderEpoch=6, isr=[0], zkVersion=6, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-3 (state.change.logger)
[2020-02-03 19:10:06,631] TRACE [Controller id=0 epoch=4] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=3, leader=-1, leaderEpoch=6, isr=[0], zkVersion=6, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-0 (state.change.logger)
[2020-02-03 19:10:06,631] TRACE [Controller id=0 epoch=4] Sending become-follower LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=3, leader=-1, leaderEpoch=6, isr=[0], zkVersion=6, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) to broker 0 for partition second_topic-5 (state.change.logger)
[2020-02-03 19:10:06,634] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition first_topic-0 (state.change.logger)
[2020-02-03 19:10:06,634] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=3, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:10:06,634] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=3, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:10:06,634] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=3, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:10:06,634] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition first_topic-2 (state.change.logger)
[2020-02-03 19:10:06,635] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition first_topic-1 (state.change.logger)
[2020-02-03 19:10:06,635] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=3, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:10:06,635] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=3, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:10:06,635] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=3, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:10:06,656] TRACE [Controller id=0 epoch=4] Changed partition second_topic-3 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:10:06,656] TRACE [Controller id=0 epoch=4] Changed partition second_topic-5 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:10:06,656] TRACE [Controller id=0 epoch=4] Changed partition second_topic-0 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:10:06,657] TRACE [Controller id=0 epoch=4] Changed partition second_topic-2 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:10:06,657] TRACE [Controller id=0 epoch=4] Changed partition second_topic-1 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:10:06,657] TRACE [Controller id=0 epoch=4] Changed partition second_topic-4 state from OfflinePartition to OfflinePartition (state.change.logger)
[2020-02-03 19:10:06,658] TRACE [Controller id=0 epoch=4] Changed partition second_topic-3 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:10:06,658] TRACE [Controller id=0 epoch=4] Changed partition second_topic-5 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:10:06,658] TRACE [Controller id=0 epoch=4] Changed partition second_topic-0 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:10:06,658] TRACE [Controller id=0 epoch=4] Changed partition second_topic-2 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:10:06,658] TRACE [Controller id=0 epoch=4] Changed partition second_topic-1 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:10:06,658] TRACE [Controller id=0 epoch=4] Changed partition second_topic-4 state from OfflinePartition to NonExistentPartition (state.change.logger)
[2020-02-03 19:10:06,660] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=3, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:10:06,660] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=3, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:10:06,660] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=3, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:10:06,660] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=3, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:10:06,660] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=3, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:10:06,660] TRACE [Controller id=0 epoch=4] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=3, leader=-2, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:10:06,694] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-3 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:10:06,694] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-5 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:10:06,694] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-0 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:10:06,694] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-2 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:10:06,694] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-1 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:10:06,694] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-4 from OnlineReplica to OfflineReplica (state.change.logger)
[2020-02-03 19:10:06,697] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-3 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:10:06,697] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-1 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:10:06,697] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-5 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:10:06,697] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-0 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:10:06,697] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-2 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:10:06,698] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-4 from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2020-02-03 19:10:06,708] TRACE [Controller id=0 epoch=4] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 0 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:10:06,717] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 4 (state.change.logger)
[2020-02-03 19:10:06,717] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 4 (state.change.logger)
[2020-02-03 19:10:06,717] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 4 (state.change.logger)
[2020-02-03 19:10:06,717] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=2, controllerEpoch=3, leader=-1, leaderEpoch=6, isr=[0], zkVersion=6, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 4 (state.change.logger)
[2020-02-03 19:10:06,717] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=4, controllerEpoch=3, leader=-1, leaderEpoch=6, isr=[0], zkVersion=6, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 4 (state.change.logger)
[2020-02-03 19:10:06,717] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=1, controllerEpoch=3, leader=-1, leaderEpoch=6, isr=[0], zkVersion=6, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 4 (state.change.logger)
[2020-02-03 19:10:06,717] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=3, controllerEpoch=3, leader=-1, leaderEpoch=6, isr=[0], zkVersion=6, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 4 (state.change.logger)
[2020-02-03 19:10:06,717] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=0, controllerEpoch=3, leader=-1, leaderEpoch=6, isr=[0], zkVersion=6, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 4 (state.change.logger)
[2020-02-03 19:10:06,717] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='second_topic', partitionIndex=5, controllerEpoch=3, leader=-1, leaderEpoch=6, isr=[0], zkVersion=6, replicas=[0], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 0 epoch 4 (state.change.logger)
[2020-02-03 19:10:06,740] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 4 starting the become-leader transition for partition first_topic-0 (state.change.logger)
[2020-02-03 19:10:06,740] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 4 starting the become-leader transition for partition first_topic-1 (state.change.logger)
[2020-02-03 19:10:06,740] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 4 starting the become-leader transition for partition first_topic-2 (state.change.logger)
[2020-02-03 19:10:06,778] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 4 with correlation id 1 for partition first_topic-0 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:10:06,786] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 4 with correlation id 1 for partition first_topic-1 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:10:06,794] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 4 with correlation id 1 for partition first_topic-2 (last update controller epoch 1) (state.change.logger)
[2020-02-03 19:10:06,795] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 4 for the become-leader transition for partition first_topic-0 (state.change.logger)
[2020-02-03 19:10:06,795] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 4 for the become-leader transition for partition first_topic-1 (state.change.logger)
[2020-02-03 19:10:06,795] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 4 for the become-leader transition for partition first_topic-2 (state.change.logger)
[2020-02-03 19:10:06,796] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 4 starting the become-follower transition for partition second_topic-4 with leader -1 (state.change.logger)
[2020-02-03 19:10:06,796] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 4 starting the become-follower transition for partition second_topic-1 with leader -1 (state.change.logger)
[2020-02-03 19:10:06,796] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 4 starting the become-follower transition for partition second_topic-5 with leader -1 (state.change.logger)
[2020-02-03 19:10:06,796] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 4 starting the become-follower transition for partition second_topic-2 with leader -1 (state.change.logger)
[2020-02-03 19:10:06,796] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 4 starting the become-follower transition for partition second_topic-3 with leader -1 (state.change.logger)
[2020-02-03 19:10:06,796] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 epoch 4 starting the become-follower transition for partition second_topic-0 with leader -1 (state.change.logger)
[2020-02-03 19:10:06,798] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 4 for partition second_topic-4 (last update controller epoch 3) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:10:06,800] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 4 for partition second_topic-1 (last update controller epoch 3) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:10:06,802] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 4 for partition second_topic-5 (last update controller epoch 3) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:10:06,805] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 4 for partition second_topic-2 (last update controller epoch 3) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:10:06,807] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 4 for partition second_topic-3 (last update controller epoch 3) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:10:06,810] ERROR [Broker id=0] Received LeaderAndIsrRequest with correlation id 1 from controller 0 epoch 4 for partition second_topic-0 (last update controller epoch 3) but cannot become follower since the new leader -1 is unavailable. (state.change.logger)
[2020-02-03 19:10:06,823] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 4 for the become-follower transition for partition second_topic-4 with leader -1 (state.change.logger)
[2020-02-03 19:10:06,823] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 4 for the become-follower transition for partition second_topic-1 with leader -1 (state.change.logger)
[2020-02-03 19:10:06,823] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 4 for the become-follower transition for partition second_topic-5 with leader -1 (state.change.logger)
[2020-02-03 19:10:06,823] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 4 for the become-follower transition for partition second_topic-2 with leader -1 (state.change.logger)
[2020-02-03 19:10:06,823] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 4 for the become-follower transition for partition second_topic-3 with leader -1 (state.change.logger)
[2020-02-03 19:10:06,823] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 0 epoch 4 for the become-follower transition for partition second_topic-0 with leader -1 (state.change.logger)
[2020-02-03 19:10:06,832] TRACE [Controller id=0 epoch=4] Received response {error_code=0,partition_errors=[{topic_name=first_topic,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=4,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=first_topic,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=first_topic,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=3,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=5,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 1 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:10:06,839] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition first_topic-0 in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 2 (state.change.logger)
[2020-02-03 19:10:06,839] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=2, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition first_topic-2 in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 2 (state.change.logger)
[2020-02-03 19:10:06,839] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='first_topic', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition first_topic-1 in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 2 (state.change.logger)
[2020-02-03 19:10:06,841] TRACE [Broker id=0] Deleted partition second_topic-2 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 2 (state.change.logger)
[2020-02-03 19:10:06,841] TRACE [Broker id=0] Deleted partition second_topic-4 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 2 (state.change.logger)
[2020-02-03 19:10:06,841] TRACE [Broker id=0] Deleted partition second_topic-1 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 2 (state.change.logger)
[2020-02-03 19:10:06,841] TRACE [Broker id=0] Deleted partition second_topic-3 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 2 (state.change.logger)
[2020-02-03 19:10:06,841] TRACE [Broker id=0] Deleted partition second_topic-0 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 2 (state.change.logger)
[2020-02-03 19:10:06,841] TRACE [Broker id=0] Deleted partition second_topic-5 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 2 (state.change.logger)
[2020-02-03 19:10:06,844] TRACE [Controller id=0 epoch=4] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 2 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:10:06,846] TRACE [Broker id=0] Deleted partition second_topic-2 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 3 (state.change.logger)
[2020-02-03 19:10:06,846] TRACE [Broker id=0] Deleted partition second_topic-4 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 3 (state.change.logger)
[2020-02-03 19:10:06,846] TRACE [Broker id=0] Deleted partition second_topic-1 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 3 (state.change.logger)
[2020-02-03 19:10:06,846] TRACE [Broker id=0] Deleted partition second_topic-3 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 3 (state.change.logger)
[2020-02-03 19:10:06,846] TRACE [Broker id=0] Deleted partition second_topic-0 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 3 (state.change.logger)
[2020-02-03 19:10:06,846] TRACE [Broker id=0] Deleted partition second_topic-5 from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 4 with correlation id 3 (state.change.logger)
[2020-02-03 19:10:06,847] TRACE [Controller id=0 epoch=4] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 3 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:10:06,852] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:10:06,853] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:10:06,853] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:10:06,853] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:10:06,853] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:10:06,853] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:10:06,853] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:10:06,853] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:10:06,853] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:10:06,853] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:10:06,853] TRACE [Broker id=0] Handling stop replica (delete=false) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:10:06,853] TRACE [Broker id=0] Finished handling stop replica (delete=false) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:10:06,855] TRACE [Controller id=0 epoch=4] Received response {error_code=0,partition_errors=[{topic_name=second_topic,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=4,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=3,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=second_topic,partition_index=5,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request STOP_REPLICA with correlation id 4 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:10:06,857] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-3 (state.change.logger)
[2020-02-03 19:10:06,873] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-3 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-3 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3.aa38a501831d4861991f6496043266b8-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-3.aa38a501831d4861991f6496043266b8-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:10:06,874] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-5 (state.change.logger)
[2020-02-03 19:10:06,883] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-5 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-5 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5.8f1d26d3cd784373accdfa18a88a4aba-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-5.8f1d26d3cd784373accdfa18a88a4aba-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:10:06,884] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-0 (state.change.logger)
[2020-02-03 19:10:06,893] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-0 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-0 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0.7263066af7044118962418a2cc825842-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-0.7263066af7044118962418a2cc825842-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:10:06,893] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-2 (state.change.logger)
[2020-02-03 19:10:06,903] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-2 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-2 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2.3e84703b64e3403d8bb9831c0105758f-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-2.3e84703b64e3403d8bb9831c0105758f-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:10:06,903] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-1 (state.change.logger)
[2020-02-03 19:10:06,913] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-1 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-1 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1.2919717fc2d94e178901613deb5f43dd-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-1.2919717fc2d94e178901613deb5f43dd-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:10:06,914] TRACE [Broker id=0] Handling stop replica (delete=true) for partition second_topic-4 (state.change.logger)
[2020-02-03 19:10:06,925] ERROR [Broker id=0] Ignoring stop replica (delete=true) for partition second_topic-4 due to storage exception (state.change.logger)
org.apache.kafka.common.errors.KafkaStorageException: Error while renaming dir for second_topic-4 in log dir C:\Nikhil\projects\Kafka\kafka\data\kafka
Caused by: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4.d6420a7bc2044844bcabd00e7d691b4a-delete
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:395)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
	at java.base/java.nio.file.Files.move(Files.java:1425)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:795)
	at kafka.log.Log.$anonfun$renameDir$2(Log.scala:966)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.log.Log.maybeHandleIOException(Log.scala:2316)
	at kafka.log.Log.renameDir(Log.scala:964)
	at kafka.log.LogManager.asyncDelete(LogManager.scala:925)
	at kafka.cluster.Partition.$anonfun$delete$1(Partition.scala:479)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:253)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:261)
	at kafka.cluster.Partition.delete(Partition.scala:470)
	at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:360)
	at kafka.server.ReplicaManager.$anonfun$stopReplicas$2(ReplicaManager.scala:404)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:321)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:977)
	at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:402)
	at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:235)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:131)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:70)
	at java.base/java.lang.Thread.run(Thread.java:830)
	Suppressed: java.nio.file.AccessDeniedException: C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4 -> C:\Nikhil\projects\Kafka\kafka\data\kafka\second_topic-4.d6420a7bc2044844bcabd00e7d691b4a-delete
		at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
		at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
		at java.base/sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:309)
		at java.base/sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:292)
		at java.base/java.nio.file.Files.move(Files.java:1425)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:792)
		... 18 more
[2020-02-03 19:10:06,927] TRACE [Controller id=0 epoch=4] Received response {error_code=0,partition_errors=[{topic_name=second_topic,partition_index=2,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=4,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=1,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=3,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=0,error_code=56,_tagged_fields={}},{topic_name=second_topic,partition_index=5,error_code=56,_tagged_fields={}}],_tagged_fields={}} for request STOP_REPLICA with correlation id 5 sent to broker 10.30.191.222:9092 (id: 0 rack: null) (state.change.logger)
[2020-02-03 19:10:06,931] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-3 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:10:06,931] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-1 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:10:06,931] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-5 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:10:06,931] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-0 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:10:06,931] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-2 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
[2020-02-03 19:10:06,931] TRACE [Controller id=0 epoch=4] Changed state of replica 0 for partition second_topic-4 from ReplicaDeletionStarted to ReplicaDeletionIneligible (state.change.logger)
